

# using knots will result in terrible predictions for a population-skewed area like alaska.
# review the available 
# https://www.census.gov/geo/maps-data/data/tallies/tractblock.html


# it is better to make knots out of the geographies you already have!


library(sqldf)
sfak <- sf1ak.101
ctract.knots <- sqldf( "select county , tract , sum( intptlat * pop100 ) / sum( pop100 ) as intptlat , sum( intptlon * pop100 ) / sum( pop100 ) as intptlon from sfak group by county , tract" )

stop( "did you deal with the aleutians across the international date line?" )
stop( "if adding knots didn't work, just collapse at the census tract level" )
stop( "fix this in all scripts" )





create survey-weighted maps of the american population 1850 until 2010 with IPUMS-USA
	https://usa.ipums.org/usa/complex_survey_vars/strata_historical.shtml
	
	


as examples to show the power of this method: find ten state-by-state maps published in reputable publications.  (1) reproduce the maps using hard state borders.  (2) reproduce the maps using sub-state geographies and within-state smoothing.  boom.



http://www.iussp.org/sites/default/files/event_call_for_papers/Abstract%20prevR.pdf
http://cybergeo.revues.org/24606



survey-weighted dasymetric maps
Thomas Lumley <tlumley@uw.edu>	Mon, Jun 2, 2014 at 5:48 PM
To: Anthony Damico <ajdamico@gmail.com>
One approach is to use survey methods to estimate means for the
smallest areas you can manage, and then treat these estimated means
and their standard errors as data for input to a smoother. Lots of
smoothers will take precision weights, which is what the standard
errors give you.

There's a 2007 paper (attached) that does this by treating
arcsine(sqrt(p.hat)) as Normal in a Bayesian spatial model, and Jon
Wakefield and I have been working on a more complicated approach which
would be overkill for your application
(http://pioneer.netserv.chula.ac.th/~sjirapha/SAE2013/Spatial%20Smoothing%20and%20Prediction%20in%20Small%20Area%20Estimation/Thomas%20Lumley.pdf)

  - thomas
[Quoted text hidden]
--
Thomas Lumley
Professor of Biostatistics







tell nathan yau about it!

data on canvas
when we publish these survey-weighted maps, i want to go over-the-top with the presentation and treat them as if they were in an art gallery.
know how at art galleries they have a little caption next to the artwork?  well i want to do that.
http://www.nga.gov/content/ngaweb/Collection/highlights/highlight106382.html

the centroid calculated should be the *population* centroid and not the geographic centroid.  so if you're mapping new york state, the population centroid should be much closer to manhattan than the center of the state.  if you just used the center of the state, that's a mistake.


write a post: the logic behind swmap:
	step 1 - let's say you know the poverty rate in the two largest counties in maryland - baltimore county & montgomery county (dc suburb), so you actually know those two points but really you've got survey data for those two points as well as everywhere else combine.  first: create a smoothed map of just those two points within the state of maryland.  display that map.
	step 2 - but you actually know three points, probably.  you know baltimore county and you know montgomery county poverty rates, but you probably also know all other counties combined.  because rural areas are often grouped together and you may not have enough survey data for every rural county but you may have those two counties and *everything else in aggregate* so you've got three points, but that third one isn't a point it's the borders.  so when you tack on the border smoothing it actually looks like this on the outside is the borders and then baltimore county and montgomery county at their centroids.
	step 3 - but the centroid of baltimore county & montgomery county, geographically, is a lie.  it's the wrong calculation as just the centroid of the county.  you need a population centroid.  for each of the points that you do know, you need a population-centered, re-weighted centroid.  calculate the centroid based on population density rather than just the geography of the county.  weighting based on population puts the centroid of baltimore county much closer to baltimore city than it does by just using the unadjusted borders of baltimore county.  and montgomery county's population center is very close to the washington dc border.  and so you would put it closer to the border rather than in the center of the county.  [[here's how to calculate a population-weighted centroid]].
	
	using the weighted population centroid and the borders of all of the other points that you don't know, now you have two centroids and one border-region.  you know three poverty rates for those two points and one border region.  creating a smoothed map based on that information should look like this: [example]  that's the logic behind swmap!



from:	 Hadley Wickham <h.wickham@gmail.com>
to:	 Anthony Damico <ajdamico@gmail.com>
cc:	 David Rae <davidbrae@gmail.com>
date:	 Mon, Jul 21, 2014 at 11:44 AM
subject:	 Re: wwhd?

I'd think of the problem in two parts:

1. take sparse raw data and convert to smooth model predictions
2. visualise those predictions

For 2., I obviously think you should use ggplot2 to visualise the
predictions, so I won't speak to that ;)  (but regardless of what you
use, I think that's the easy part)

For 1., you'll need to break it down into steps:

a) Fit the model. I think the approach that Josh Katz takes here
(k-nearest neighbours gaussian smoothing,
http://www4.ncsu.edu/~jakatz2/files/dialectposter.png) seems
reasonable, but you could also use kriging, or something more
sophisticated.  You might want to make this user-selectable, but I'd
start with one model.

b) Generate grid (optionally constrained to more complex shape like
map of states). For this, either generate a grid based on the range of
lat & lon in the input data, or allow the user to supply a map or sp
object to specify where to take the grid. I haven't done this in the
past, but I suspect the raster package is mostly likely to help here.

c) Generate predictions from grid (capturing both mean & standard
error/local density). This should be relatively simple - providing
your using a model the follows the standard interface, this should
just be a matter of predict(model, newdata, se = T)

Hadley